{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc10fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import unique\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46bab103",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\49173\\Desktop\\Data\\stockxf2.csv\")\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4290272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productId</th>\n",
       "      <th>orderDate</th>\n",
       "      <th>salePrice</th>\n",
       "      <th>shoeSize</th>\n",
       "      <th>brand</th>\n",
       "      <th>sneakerName</th>\n",
       "      <th>colorway</th>\n",
       "      <th>retailPrice</th>\n",
       "      <th>releaseDate</th>\n",
       "      <th>salesThisPeriod</th>\n",
       "      <th>hype</th>\n",
       "      <th>days</th>\n",
       "      <th>collaboration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55a9ccfe-129a-438a-9989-b877c2a66279</td>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>300</td>\n",
       "      <td>9.5</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>Jordan 4 Retro Military Black</td>\n",
       "      <td>White/Black-Neutral Grey</td>\n",
       "      <td>210</td>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>2476</td>\n",
       "      <td>10743</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55a9ccfe-129a-438a-9989-b877c2a66279</td>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>300</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>Jordan 4 Retro Military Black</td>\n",
       "      <td>White/Black-Neutral Grey</td>\n",
       "      <td>210</td>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>2476</td>\n",
       "      <td>10743</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55a9ccfe-129a-438a-9989-b877c2a66279</td>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>382</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>Jordan 4 Retro Military Black</td>\n",
       "      <td>White/Black-Neutral Grey</td>\n",
       "      <td>210</td>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>2476</td>\n",
       "      <td>10743</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55a9ccfe-129a-438a-9989-b877c2a66279</td>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>304</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>Jordan 4 Retro Military Black</td>\n",
       "      <td>White/Black-Neutral Grey</td>\n",
       "      <td>210</td>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>2476</td>\n",
       "      <td>10743</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55a9ccfe-129a-438a-9989-b877c2a66279</td>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>390</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>Jordan 4 Retro Military Black</td>\n",
       "      <td>White/Black-Neutral Grey</td>\n",
       "      <td>210</td>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>2476</td>\n",
       "      <td>10743</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192880</th>\n",
       "      <td>3774e829-99f4-46d6-bb94-f9e19ad55ad3</td>\n",
       "      <td>2022-03-18</td>\n",
       "      <td>384</td>\n",
       "      <td>10.0</td>\n",
       "      <td>adidas</td>\n",
       "      <td>adidas Yeezy Boost 350 V2 Sesame</td>\n",
       "      <td>Sesame/Sesame/Sesame</td>\n",
       "      <td>220</td>\n",
       "      <td>2018-11-23</td>\n",
       "      <td>13</td>\n",
       "      <td>5366</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192881</th>\n",
       "      <td>3774e829-99f4-46d6-bb94-f9e19ad55ad3</td>\n",
       "      <td>2022-03-18</td>\n",
       "      <td>366</td>\n",
       "      <td>12.0</td>\n",
       "      <td>adidas</td>\n",
       "      <td>adidas Yeezy Boost 350 V2 Sesame</td>\n",
       "      <td>Sesame/Sesame/Sesame</td>\n",
       "      <td>220</td>\n",
       "      <td>2018-11-23</td>\n",
       "      <td>13</td>\n",
       "      <td>5366</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192882</th>\n",
       "      <td>3774e829-99f4-46d6-bb94-f9e19ad55ad3</td>\n",
       "      <td>2022-03-18</td>\n",
       "      <td>420</td>\n",
       "      <td>10.5</td>\n",
       "      <td>adidas</td>\n",
       "      <td>adidas Yeezy Boost 350 V2 Sesame</td>\n",
       "      <td>Sesame/Sesame/Sesame</td>\n",
       "      <td>220</td>\n",
       "      <td>2018-11-23</td>\n",
       "      <td>13</td>\n",
       "      <td>5366</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192883</th>\n",
       "      <td>3774e829-99f4-46d6-bb94-f9e19ad55ad3</td>\n",
       "      <td>2022-03-18</td>\n",
       "      <td>357</td>\n",
       "      <td>11.0</td>\n",
       "      <td>adidas</td>\n",
       "      <td>adidas Yeezy Boost 350 V2 Sesame</td>\n",
       "      <td>Sesame/Sesame/Sesame</td>\n",
       "      <td>220</td>\n",
       "      <td>2018-11-23</td>\n",
       "      <td>13</td>\n",
       "      <td>5366</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192884</th>\n",
       "      <td>3774e829-99f4-46d6-bb94-f9e19ad55ad3</td>\n",
       "      <td>2022-03-18</td>\n",
       "      <td>383</td>\n",
       "      <td>9.0</td>\n",
       "      <td>adidas</td>\n",
       "      <td>adidas Yeezy Boost 350 V2 Sesame</td>\n",
       "      <td>Sesame/Sesame/Sesame</td>\n",
       "      <td>220</td>\n",
       "      <td>2018-11-23</td>\n",
       "      <td>13</td>\n",
       "      <td>5366</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192885 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   productId   orderDate  salePrice  shoeSize  \\\n",
       "0       55a9ccfe-129a-438a-9989-b877c2a66279  2022-05-26        300       9.5   \n",
       "1       55a9ccfe-129a-438a-9989-b877c2a66279  2022-05-26        300       9.0   \n",
       "2       55a9ccfe-129a-438a-9989-b877c2a66279  2022-05-26        382      10.0   \n",
       "3       55a9ccfe-129a-438a-9989-b877c2a66279  2022-05-26        304       8.0   \n",
       "4       55a9ccfe-129a-438a-9989-b877c2a66279  2022-05-26        390      12.0   \n",
       "...                                      ...         ...        ...       ...   \n",
       "192880  3774e829-99f4-46d6-bb94-f9e19ad55ad3  2022-03-18        384      10.0   \n",
       "192881  3774e829-99f4-46d6-bb94-f9e19ad55ad3  2022-03-18        366      12.0   \n",
       "192882  3774e829-99f4-46d6-bb94-f9e19ad55ad3  2022-03-18        420      10.5   \n",
       "192883  3774e829-99f4-46d6-bb94-f9e19ad55ad3  2022-03-18        357      11.0   \n",
       "192884  3774e829-99f4-46d6-bb94-f9e19ad55ad3  2022-03-18        383       9.0   \n",
       "\n",
       "         brand                       sneakerName                  colorway  \\\n",
       "0       Jordan     Jordan 4 Retro Military Black  White/Black-Neutral Grey   \n",
       "1       Jordan     Jordan 4 Retro Military Black  White/Black-Neutral Grey   \n",
       "2       Jordan     Jordan 4 Retro Military Black  White/Black-Neutral Grey   \n",
       "3       Jordan     Jordan 4 Retro Military Black  White/Black-Neutral Grey   \n",
       "4       Jordan     Jordan 4 Retro Military Black  White/Black-Neutral Grey   \n",
       "...        ...                               ...                       ...   \n",
       "192880  adidas  adidas Yeezy Boost 350 V2 Sesame      Sesame/Sesame/Sesame   \n",
       "192881  adidas  adidas Yeezy Boost 350 V2 Sesame      Sesame/Sesame/Sesame   \n",
       "192882  adidas  adidas Yeezy Boost 350 V2 Sesame      Sesame/Sesame/Sesame   \n",
       "192883  adidas  adidas Yeezy Boost 350 V2 Sesame      Sesame/Sesame/Sesame   \n",
       "192884  adidas  adidas Yeezy Boost 350 V2 Sesame      Sesame/Sesame/Sesame   \n",
       "\n",
       "        retailPrice releaseDate  salesThisPeriod   hype  days  collaboration  \n",
       "0               210  2022-05-21             2476  10743     5              1  \n",
       "1               210  2022-05-21             2476  10743     5              1  \n",
       "2               210  2022-05-21             2476  10743     5              1  \n",
       "3               210  2022-05-21             2476  10743     5              1  \n",
       "4               210  2022-05-21             2476  10743     5              1  \n",
       "...             ...         ...              ...    ...   ...            ...  \n",
       "192880          220  2018-11-23               13   5366  1211              1  \n",
       "192881          220  2018-11-23               13   5366  1211              1  \n",
       "192882          220  2018-11-23               13   5366  1211              1  \n",
       "192883          220  2018-11-23               13   5366  1211              1  \n",
       "192884          220  2018-11-23               13   5366  1211              1  \n",
       "\n",
       "[192885 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56742a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['log_hype'] = np.log(df['hype']+1)\n",
    "df['log_resalePrice'] = np.log(df['salePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a609b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['productId', \n",
    "              'orderDate',\n",
    "              'sneakerName',\n",
    "              'colorway', \n",
    "              'releaseDate', \n",
    "              \"hype\", \n",
    "              'salePrice'],\n",
    "              axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44273c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shoeSize</th>\n",
       "      <th>brand</th>\n",
       "      <th>retailPrice</th>\n",
       "      <th>salesThisPeriod</th>\n",
       "      <th>days</th>\n",
       "      <th>collaboration</th>\n",
       "      <th>log_resalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.5</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5.703782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5.703782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5.945421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.0</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5.717028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.0</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5.966147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192880</th>\n",
       "      <td>10.0</td>\n",
       "      <td>adidas</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>5.950643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192881</th>\n",
       "      <td>12.0</td>\n",
       "      <td>adidas</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>5.902633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192882</th>\n",
       "      <td>10.5</td>\n",
       "      <td>adidas</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>6.040255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192883</th>\n",
       "      <td>11.0</td>\n",
       "      <td>adidas</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>5.877736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192884</th>\n",
       "      <td>9.0</td>\n",
       "      <td>adidas</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>5.948035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192885 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        shoeSize   brand  retailPrice  salesThisPeriod  days  collaboration  \\\n",
       "0            9.5  Jordan          210             2476     5              1   \n",
       "1            9.0  Jordan          210             2476     5              1   \n",
       "2           10.0  Jordan          210             2476     5              1   \n",
       "3            8.0  Jordan          210             2476     5              1   \n",
       "4           12.0  Jordan          210             2476     5              1   \n",
       "...          ...     ...          ...              ...   ...            ...   \n",
       "192880      10.0  adidas          220               13  1211              1   \n",
       "192881      12.0  adidas          220               13  1211              1   \n",
       "192882      10.5  adidas          220               13  1211              1   \n",
       "192883      11.0  adidas          220               13  1211              1   \n",
       "192884       9.0  adidas          220               13  1211              1   \n",
       "\n",
       "        log_resalePrice  \n",
       "0              5.703782  \n",
       "1              5.703782  \n",
       "2              5.945421  \n",
       "3              5.717028  \n",
       "4              5.966147  \n",
       "...                 ...  \n",
       "192880         5.950643  \n",
       "192881         5.902633  \n",
       "192882         6.040255  \n",
       "192883         5.877736  \n",
       "192884         5.948035  \n",
       "\n",
       "[192885 rows x 7 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ab43868",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.get_dummies(df[\"brand\"], prefix='brand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8858088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['brand'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bf4d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 = pd.get_dummies(df[\"sneakerName\"], prefix='SN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49e5f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(['sneakerName'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4273f4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shoeSize</th>\n",
       "      <th>retailPrice</th>\n",
       "      <th>salesThisPeriod</th>\n",
       "      <th>days</th>\n",
       "      <th>collaboration</th>\n",
       "      <th>log_resalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.5</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5.703782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5.703782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5.945421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.0</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5.717028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.0</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5.966147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192880</th>\n",
       "      <td>10.0</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>5.950643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192881</th>\n",
       "      <td>12.0</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>5.902633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192882</th>\n",
       "      <td>10.5</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>6.040255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192883</th>\n",
       "      <td>11.0</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>5.877736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192884</th>\n",
       "      <td>9.0</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>5.948035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192885 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        shoeSize  retailPrice  salesThisPeriod  days  collaboration  \\\n",
       "0            9.5          210             2476     5              1   \n",
       "1            9.0          210             2476     5              1   \n",
       "2           10.0          210             2476     5              1   \n",
       "3            8.0          210             2476     5              1   \n",
       "4           12.0          210             2476     5              1   \n",
       "...          ...          ...              ...   ...            ...   \n",
       "192880      10.0          220               13  1211              1   \n",
       "192881      12.0          220               13  1211              1   \n",
       "192882      10.5          220               13  1211              1   \n",
       "192883      11.0          220               13  1211              1   \n",
       "192884       9.0          220               13  1211              1   \n",
       "\n",
       "        log_resalePrice  \n",
       "0              5.703782  \n",
       "1              5.703782  \n",
       "2              5.945421  \n",
       "3              5.717028  \n",
       "4              5.966147  \n",
       "...                 ...  \n",
       "192880         5.950643  \n",
       "192881         5.902633  \n",
       "192882         6.040255  \n",
       "192883         5.877736  \n",
       "192884         5.948035  \n",
       "\n",
       "[192885 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0296e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6118a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.join(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c61c5eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shoeSize</th>\n",
       "      <th>retailPrice</th>\n",
       "      <th>salesThisPeriod</th>\n",
       "      <th>days</th>\n",
       "      <th>collaboration</th>\n",
       "      <th>log_resalePrice</th>\n",
       "      <th>brand_Alexander McQueen</th>\n",
       "      <th>brand_BAPE</th>\n",
       "      <th>brand_Common Projects</th>\n",
       "      <th>brand_Converse</th>\n",
       "      <th>...</th>\n",
       "      <th>brand_Jordan</th>\n",
       "      <th>brand_MSCHF</th>\n",
       "      <th>brand_New Balance</th>\n",
       "      <th>brand_Nike</th>\n",
       "      <th>brand_Puma</th>\n",
       "      <th>brand_Reebok</th>\n",
       "      <th>brand_Salomon</th>\n",
       "      <th>brand_Under Armour</th>\n",
       "      <th>brand_Vans</th>\n",
       "      <th>brand_adidas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.5</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5.703782</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5.703782</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5.945421</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.0</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5.717028</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.0</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5.966147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192880</th>\n",
       "      <td>10.0</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>5.950643</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192881</th>\n",
       "      <td>12.0</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>5.902633</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192882</th>\n",
       "      <td>10.5</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>6.040255</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192883</th>\n",
       "      <td>11.0</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>5.877736</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192884</th>\n",
       "      <td>9.0</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>5.948035</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192885 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        shoeSize  retailPrice  salesThisPeriod  days  collaboration  \\\n",
       "0            9.5          210             2476     5              1   \n",
       "1            9.0          210             2476     5              1   \n",
       "2           10.0          210             2476     5              1   \n",
       "3            8.0          210             2476     5              1   \n",
       "4           12.0          210             2476     5              1   \n",
       "...          ...          ...              ...   ...            ...   \n",
       "192880      10.0          220               13  1211              1   \n",
       "192881      12.0          220               13  1211              1   \n",
       "192882      10.5          220               13  1211              1   \n",
       "192883      11.0          220               13  1211              1   \n",
       "192884       9.0          220               13  1211              1   \n",
       "\n",
       "        log_resalePrice  brand_Alexander McQueen  brand_BAPE  \\\n",
       "0              5.703782                        0           0   \n",
       "1              5.703782                        0           0   \n",
       "2              5.945421                        0           0   \n",
       "3              5.717028                        0           0   \n",
       "4              5.966147                        0           0   \n",
       "...                 ...                      ...         ...   \n",
       "192880         5.950643                        0           0   \n",
       "192881         5.902633                        0           0   \n",
       "192882         6.040255                        0           0   \n",
       "192883         5.877736                        0           0   \n",
       "192884         5.948035                        0           0   \n",
       "\n",
       "        brand_Common Projects  brand_Converse  ...  brand_Jordan  brand_MSCHF  \\\n",
       "0                           0               0  ...             1            0   \n",
       "1                           0               0  ...             1            0   \n",
       "2                           0               0  ...             1            0   \n",
       "3                           0               0  ...             1            0   \n",
       "4                           0               0  ...             1            0   \n",
       "...                       ...             ...  ...           ...          ...   \n",
       "192880                      0               0  ...             0            0   \n",
       "192881                      0               0  ...             0            0   \n",
       "192882                      0               0  ...             0            0   \n",
       "192883                      0               0  ...             0            0   \n",
       "192884                      0               0  ...             0            0   \n",
       "\n",
       "        brand_New Balance  brand_Nike  brand_Puma  brand_Reebok  \\\n",
       "0                       0           0           0             0   \n",
       "1                       0           0           0             0   \n",
       "2                       0           0           0             0   \n",
       "3                       0           0           0             0   \n",
       "4                       0           0           0             0   \n",
       "...                   ...         ...         ...           ...   \n",
       "192880                  0           0           0             0   \n",
       "192881                  0           0           0             0   \n",
       "192882                  0           0           0             0   \n",
       "192883                  0           0           0             0   \n",
       "192884                  0           0           0             0   \n",
       "\n",
       "        brand_Salomon  brand_Under Armour  brand_Vans  brand_adidas  \n",
       "0                   0                   0           0             0  \n",
       "1                   0                   0           0             0  \n",
       "2                   0                   0           0             0  \n",
       "3                   0                   0           0             0  \n",
       "4                   0                   0           0             0  \n",
       "...               ...                 ...         ...           ...  \n",
       "192880              0                   0           0             1  \n",
       "192881              0                   0           0             1  \n",
       "192882              0                   0           0             1  \n",
       "192883              0                   0           0             1  \n",
       "192884              0                   0           0             1  \n",
       "\n",
       "[192885 rows x 21 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82376efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['constant', 'shoeSize', 'retailPrice', 'salesThisPeriod', 'days', 'collaboration', 'brand_Alexander McQueen', 'brand_BAPE', 'brand_Common Projects', 'brand_Converse', 'brand_Crocs', 'brand_Jordan', 'brand_MSCHF', 'brand_New Balance', 'brand_Nike', 'brand_Puma', 'brand_Reebok', 'brand_Salomon', 'brand_Under Armour', 'brand_Vans', 'brand_adidas']\n"
     ]
    }
   ],
   "source": [
    "dfnames = df.copy()\n",
    "dfnames = dfnames.drop(['log_resalePrice'], axis = 1)\n",
    "\n",
    "columns = dfnames.columns.tolist()\n",
    "columns.insert(0, 'constant')\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7ce940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('log_resalePrice',axis =1).values\n",
    "y = df['log_resalePrice'].values\n",
    "\n",
    "#splitting Train and Test \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ca1e7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\49173\\AppData\\Local\\Temp\\ipykernel_21756\\2855883272.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_train = s_scaler.fit_transform(X_train.astype(np.float))\n",
      "C:\\Users\\49173\\AppData\\Local\\Temp\\ipykernel_21756\\2855883272.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_test = s_scaler.transform(X_test.astype(np.float))\n"
     ]
    }
   ],
   "source": [
    "s_scaler = StandardScaler()\n",
    "X_train = s_scaler.fit_transform(X_train.astype(np.float))\n",
    "X_test = s_scaler.transform(X_test.astype(np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42a4c631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Neural Network Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f824ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# having 9 neurons is based on the number of available features\n",
    "model = Sequential()\n",
    "model.add(Dense(9,activation='relu'))\n",
    "model.add(Dense(18,activation='relu'))\n",
    "model.add(Dense(18,activation='relu'))\n",
    "model.add(Dense(9,activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='Adam',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5487051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1055/1055 [==============================] - 9s 3ms/step - loss: 2.0089 - val_loss: 0.1462\n",
      "Epoch 2/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.1335 - val_loss: 0.1284\n",
      "Epoch 3/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.1245 - val_loss: 0.1241\n",
      "Epoch 4/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.1192 - val_loss: 0.1202\n",
      "Epoch 5/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.1136 - val_loss: 0.1141\n",
      "Epoch 6/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.1104 - val_loss: 0.1099\n",
      "Epoch 7/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.1077 - val_loss: 0.1086\n",
      "Epoch 8/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.1051 - val_loss: 0.1051\n",
      "Epoch 9/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.1035 - val_loss: 0.1029\n",
      "Epoch 10/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.1022 - val_loss: 0.1046\n",
      "Epoch 11/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.1008 - val_loss: 0.1015\n",
      "Epoch 12/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0997 - val_loss: 0.1022\n",
      "Epoch 13/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0984 - val_loss: 0.0992\n",
      "Epoch 14/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0972 - val_loss: 0.0979\n",
      "Epoch 15/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0965 - val_loss: 0.0960\n",
      "Epoch 16/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0957 - val_loss: 0.1049\n",
      "Epoch 17/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0950 - val_loss: 0.0977\n",
      "Epoch 18/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0943 - val_loss: 0.0941\n",
      "Epoch 19/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0937 - val_loss: 0.0951\n",
      "Epoch 20/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0930 - val_loss: 0.0934\n",
      "Epoch 21/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0923 - val_loss: 0.0956\n",
      "Epoch 22/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0920 - val_loss: 0.0932\n",
      "Epoch 23/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0919 - val_loss: 0.0909\n",
      "Epoch 24/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0909 - val_loss: 0.0908\n",
      "Epoch 25/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0907 - val_loss: 0.0915\n",
      "Epoch 26/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0900 - val_loss: 0.0917\n",
      "Epoch 27/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0898 - val_loss: 0.0903\n",
      "Epoch 28/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0895 - val_loss: 0.0957\n",
      "Epoch 29/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0890 - val_loss: 0.0892\n",
      "Epoch 30/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0888 - val_loss: 0.0905\n",
      "Epoch 31/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0890 - val_loss: 0.0918\n",
      "Epoch 32/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0885 - val_loss: 0.0885\n",
      "Epoch 33/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0882 - val_loss: 0.0879\n",
      "Epoch 34/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0880 - val_loss: 0.0917\n",
      "Epoch 35/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0877 - val_loss: 0.0880\n",
      "Epoch 36/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0876 - val_loss: 0.0922\n",
      "Epoch 37/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0873 - val_loss: 0.0881\n",
      "Epoch 38/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0871 - val_loss: 0.0931\n",
      "Epoch 39/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0866 - val_loss: 0.0894\n",
      "Epoch 40/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0866 - val_loss: 0.0864\n",
      "Epoch 41/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0862 - val_loss: 0.0886\n",
      "Epoch 42/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0861 - val_loss: 0.0973\n",
      "Epoch 43/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0856 - val_loss: 0.0865\n",
      "Epoch 44/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0853 - val_loss: 0.0849\n",
      "Epoch 45/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0848 - val_loss: 0.0881\n",
      "Epoch 46/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0847 - val_loss: 0.0869\n",
      "Epoch 47/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0844 - val_loss: 0.0851\n",
      "Epoch 48/400\n",
      "1055/1055 [==============================] - 5s 4ms/step - loss: 0.0845 - val_loss: 0.0840\n",
      "Epoch 49/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0840 - val_loss: 0.0870\n",
      "Epoch 50/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0838 - val_loss: 0.0841\n",
      "Epoch 51/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0833 - val_loss: 0.0838\n",
      "Epoch 52/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0832 - val_loss: 0.0833\n",
      "Epoch 53/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0826 - val_loss: 0.0858\n",
      "Epoch 54/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0829 - val_loss: 0.0848\n",
      "Epoch 55/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0825 - val_loss: 0.0819\n",
      "Epoch 56/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0826 - val_loss: 0.0871\n",
      "Epoch 57/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0822 - val_loss: 0.0844\n",
      "Epoch 58/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0818 - val_loss: 0.0841\n",
      "Epoch 59/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0820 - val_loss: 0.0811\n",
      "Epoch 60/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0815 - val_loss: 0.0810\n",
      "Epoch 61/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0814 - val_loss: 0.0813\n",
      "Epoch 62/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0815 - val_loss: 0.0854\n",
      "Epoch 63/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0811 - val_loss: 0.0809\n",
      "Epoch 64/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0809 - val_loss: 0.0820\n",
      "Epoch 65/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0811 - val_loss: 0.0811\n",
      "Epoch 66/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0810 - val_loss: 0.0806\n",
      "Epoch 67/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0805 - val_loss: 0.0809\n",
      "Epoch 68/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0808 - val_loss: 0.0805\n",
      "Epoch 69/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0801 - val_loss: 0.0807\n",
      "Epoch 70/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0800 - val_loss: 0.0837\n",
      "Epoch 71/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0797 - val_loss: 0.0808\n",
      "Epoch 72/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0801 - val_loss: 0.0806\n",
      "Epoch 73/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0797 - val_loss: 0.0817\n",
      "Epoch 74/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0794 - val_loss: 0.0803\n",
      "Epoch 75/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0789 - val_loss: 0.0781\n",
      "Epoch 76/400\n",
      "1055/1055 [==============================] - 5s 5ms/step - loss: 0.0788 - val_loss: 0.0782\n",
      "Epoch 77/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0788 - val_loss: 0.0817\n",
      "Epoch 78/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0786 - val_loss: 0.0786\n",
      "Epoch 79/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0783 - val_loss: 0.0773\n",
      "Epoch 80/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0783 - val_loss: 0.0820\n",
      "Epoch 81/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0780 - val_loss: 0.0798\n",
      "Epoch 82/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0777 - val_loss: 0.0767\n",
      "Epoch 83/400\n",
      "1055/1055 [==============================] - 5s 4ms/step - loss: 0.0776 - val_loss: 0.0771\n",
      "Epoch 84/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0774 - val_loss: 0.0760\n",
      "Epoch 85/400\n",
      "1055/1055 [==============================] - 5s 5ms/step - loss: 0.0775 - val_loss: 0.0808\n",
      "Epoch 86/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0770 - val_loss: 0.0775\n",
      "Epoch 87/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0770 - val_loss: 0.0779\n",
      "Epoch 88/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0769 - val_loss: 0.0789\n",
      "Epoch 89/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0765 - val_loss: 0.0773\n",
      "Epoch 90/400\n",
      "1055/1055 [==============================] - 5s 4ms/step - loss: 0.0768 - val_loss: 0.0760\n",
      "Epoch 91/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0763 - val_loss: 0.0810\n",
      "Epoch 92/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0766 - val_loss: 0.0784\n",
      "Epoch 93/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0765 - val_loss: 0.0791\n",
      "Epoch 94/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0762 - val_loss: 0.0771\n",
      "Epoch 95/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0761 - val_loss: 0.0761\n",
      "Epoch 96/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0760 - val_loss: 0.0783\n",
      "Epoch 97/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0753 - val_loss: 0.0755\n",
      "Epoch 98/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0754 - val_loss: 0.0786\n",
      "Epoch 99/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0749 - val_loss: 0.0769\n",
      "Epoch 100/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0753 - val_loss: 0.0767\n",
      "Epoch 101/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0746 - val_loss: 0.0741\n",
      "Epoch 102/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0748 - val_loss: 0.0743\n",
      "Epoch 103/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0748 - val_loss: 0.0738\n",
      "Epoch 104/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0740 - val_loss: 0.0779\n",
      "Epoch 105/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0739 - val_loss: 0.0778\n",
      "Epoch 106/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0736 - val_loss: 0.0724\n",
      "Epoch 107/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0730 - val_loss: 0.0740\n",
      "Epoch 108/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0732 - val_loss: 0.0726\n",
      "Epoch 109/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0728 - val_loss: 0.0715\n",
      "Epoch 110/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0727 - val_loss: 0.0728\n",
      "Epoch 111/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0727 - val_loss: 0.0718\n",
      "Epoch 112/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0726 - val_loss: 0.0735\n",
      "Epoch 113/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0723 - val_loss: 0.0711\n",
      "Epoch 114/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0724 - val_loss: 0.0713\n",
      "Epoch 115/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0723 - val_loss: 0.0725\n",
      "Epoch 116/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0719 - val_loss: 0.0719\n",
      "Epoch 117/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0728 - val_loss: 0.0717\n",
      "Epoch 118/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0720 - val_loss: 0.0714\n",
      "Epoch 119/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0716 - val_loss: 0.0715\n",
      "Epoch 120/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0721 - val_loss: 0.0729\n",
      "Epoch 121/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0717 - val_loss: 0.0719\n",
      "Epoch 122/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0716 - val_loss: 0.0707\n",
      "Epoch 123/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0717 - val_loss: 0.0721\n",
      "Epoch 124/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0713 - val_loss: 0.0717\n",
      "Epoch 125/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0709 - val_loss: 0.0711\n",
      "Epoch 126/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0710 - val_loss: 0.0735\n",
      "Epoch 127/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0708 - val_loss: 0.0717\n",
      "Epoch 128/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0707 - val_loss: 0.0748\n",
      "Epoch 129/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0705 - val_loss: 0.0746\n",
      "Epoch 130/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0703 - val_loss: 0.0709\n",
      "Epoch 131/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0700 - val_loss: 0.0691\n",
      "Epoch 132/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0701 - val_loss: 0.0697\n",
      "Epoch 133/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0697 - val_loss: 0.0706\n",
      "Epoch 134/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0697 - val_loss: 0.0690\n",
      "Epoch 135/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0697 - val_loss: 0.0681\n",
      "Epoch 136/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0692 - val_loss: 0.0715\n",
      "Epoch 137/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0696 - val_loss: 0.0679\n",
      "Epoch 138/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0693 - val_loss: 0.0677\n",
      "Epoch 139/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0695 - val_loss: 0.0689\n",
      "Epoch 140/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0694 - val_loss: 0.0690\n",
      "Epoch 141/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0691 - val_loss: 0.0743\n",
      "Epoch 142/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0691 - val_loss: 0.0682\n",
      "Epoch 143/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0691 - val_loss: 0.0704\n",
      "Epoch 144/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0686 - val_loss: 0.0690\n",
      "Epoch 145/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0691 - val_loss: 0.0677\n",
      "Epoch 146/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0689 - val_loss: 0.0704\n",
      "Epoch 147/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0685 - val_loss: 0.0691\n",
      "Epoch 148/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0686 - val_loss: 0.0692\n",
      "Epoch 149/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0682 - val_loss: 0.0703\n",
      "Epoch 150/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0686 - val_loss: 0.0691\n",
      "Epoch 151/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0683 - val_loss: 0.0679\n",
      "Epoch 152/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0683 - val_loss: 0.0680\n",
      "Epoch 153/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0681 - val_loss: 0.0674\n",
      "Epoch 154/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0684 - val_loss: 0.0664\n",
      "Epoch 155/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0678 - val_loss: 0.0679\n",
      "Epoch 156/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0682 - val_loss: 0.0688\n",
      "Epoch 157/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0681 - val_loss: 0.0670\n",
      "Epoch 158/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0676 - val_loss: 0.0715\n",
      "Epoch 159/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0679 - val_loss: 0.0727\n",
      "Epoch 160/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0677 - val_loss: 0.0664\n",
      "Epoch 161/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0680 - val_loss: 0.0725\n",
      "Epoch 162/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0679 - val_loss: 0.0704\n",
      "Epoch 163/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0673 - val_loss: 0.0679\n",
      "Epoch 164/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0673 - val_loss: 0.0667\n",
      "Epoch 165/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0672 - val_loss: 0.0705\n",
      "Epoch 166/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0671 - val_loss: 0.0701\n",
      "Epoch 167/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0671 - val_loss: 0.0718\n",
      "Epoch 168/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0668 - val_loss: 0.0681\n",
      "Epoch 169/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0672 - val_loss: 0.0679\n",
      "Epoch 170/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0668 - val_loss: 0.0660\n",
      "Epoch 171/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0670 - val_loss: 0.0670\n",
      "Epoch 172/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0668 - val_loss: 0.0683\n",
      "Epoch 173/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0668 - val_loss: 0.0650\n",
      "Epoch 174/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0665 - val_loss: 0.0685\n",
      "Epoch 175/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0667 - val_loss: 0.0646\n",
      "Epoch 176/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0667 - val_loss: 0.0657\n",
      "Epoch 177/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0663 - val_loss: 0.0676\n",
      "Epoch 178/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0666 - val_loss: 0.0658\n",
      "Epoch 179/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0664 - val_loss: 0.0658\n",
      "Epoch 180/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0661 - val_loss: 0.0663\n",
      "Epoch 181/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0657 - val_loss: 0.0644\n",
      "Epoch 182/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0661 - val_loss: 0.0676\n",
      "Epoch 183/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0661 - val_loss: 0.0641\n",
      "Epoch 184/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0665 - val_loss: 0.0651\n",
      "Epoch 185/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0660 - val_loss: 0.0682\n",
      "Epoch 186/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0657 - val_loss: 0.0645\n",
      "Epoch 187/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0661 - val_loss: 0.0650\n",
      "Epoch 188/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0659 - val_loss: 0.0657\n",
      "Epoch 189/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0656 - val_loss: 0.0685\n",
      "Epoch 190/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0657 - val_loss: 0.0648\n",
      "Epoch 191/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0660 - val_loss: 0.0639\n",
      "Epoch 192/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0656 - val_loss: 0.0662\n",
      "Epoch 193/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0654 - val_loss: 0.0693\n",
      "Epoch 194/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0655 - val_loss: 0.0652\n",
      "Epoch 195/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0655 - val_loss: 0.0649\n",
      "Epoch 196/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0656 - val_loss: 0.0647\n",
      "Epoch 197/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0653 - val_loss: 0.0663\n",
      "Epoch 198/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0657 - val_loss: 0.0649\n",
      "Epoch 199/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0655 - val_loss: 0.0672\n",
      "Epoch 200/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0651 - val_loss: 0.0655\n",
      "Epoch 201/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0655 - val_loss: 0.0635\n",
      "Epoch 202/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0654 - val_loss: 0.0655\n",
      "Epoch 203/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0651 - val_loss: 0.0657\n",
      "Epoch 204/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0655 - val_loss: 0.0754\n",
      "Epoch 205/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0651 - val_loss: 0.0663\n",
      "Epoch 206/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0649 - val_loss: 0.0661\n",
      "Epoch 207/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0651 - val_loss: 0.0641\n",
      "Epoch 208/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0650 - val_loss: 0.0654\n",
      "Epoch 209/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0652 - val_loss: 0.0695\n",
      "Epoch 210/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0650 - val_loss: 0.0636\n",
      "Epoch 211/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0651 - val_loss: 0.0642\n",
      "Epoch 212/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0650 - val_loss: 0.0642\n",
      "Epoch 213/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0649 - val_loss: 0.0653\n",
      "Epoch 214/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0645 - val_loss: 0.0632\n",
      "Epoch 215/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0647 - val_loss: 0.0651\n",
      "Epoch 216/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0647 - val_loss: 0.0619\n",
      "Epoch 217/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0652 - val_loss: 0.0637\n",
      "Epoch 218/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0645 - val_loss: 0.0654\n",
      "Epoch 219/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0646 - val_loss: 0.0649\n",
      "Epoch 220/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0647 - val_loss: 0.0631\n",
      "Epoch 221/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0645 - val_loss: 0.0672\n",
      "Epoch 222/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0649 - val_loss: 0.0645\n",
      "Epoch 223/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0643 - val_loss: 0.0633\n",
      "Epoch 224/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0644 - val_loss: 0.0648\n",
      "Epoch 225/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0646 - val_loss: 0.0640\n",
      "Epoch 226/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0644 - val_loss: 0.0658\n",
      "Epoch 227/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0645 - val_loss: 0.0659\n",
      "Epoch 228/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0645 - val_loss: 0.0669\n",
      "Epoch 229/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0641 - val_loss: 0.0635\n",
      "Epoch 230/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0644 - val_loss: 0.0622\n",
      "Epoch 231/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0642 - val_loss: 0.0630\n",
      "Epoch 232/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0643 - val_loss: 0.0651\n",
      "Epoch 233/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0639 - val_loss: 0.0645\n",
      "Epoch 234/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0641 - val_loss: 0.0659\n",
      "Epoch 235/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0641 - val_loss: 0.0633\n",
      "Epoch 236/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0642 - val_loss: 0.0626\n",
      "Epoch 237/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0643 - val_loss: 0.0676\n",
      "Epoch 238/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0639 - val_loss: 0.0632\n",
      "Epoch 239/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0638 - val_loss: 0.0627\n",
      "Epoch 240/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0639 - val_loss: 0.0628\n",
      "Epoch 241/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0641 - val_loss: 0.0642\n",
      "Epoch 242/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0638 - val_loss: 0.0635\n",
      "Epoch 243/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0640 - val_loss: 0.0656\n",
      "Epoch 244/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0637 - val_loss: 0.0653\n",
      "Epoch 245/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0636 - val_loss: 0.0621\n",
      "Epoch 246/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0640 - val_loss: 0.0611\n",
      "Epoch 247/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0638 - val_loss: 0.0639\n",
      "Epoch 248/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0635 - val_loss: 0.0645\n",
      "Epoch 249/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0641 - val_loss: 0.0616\n",
      "Epoch 250/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0637 - val_loss: 0.0640\n",
      "Epoch 251/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0637 - val_loss: 0.0622\n",
      "Epoch 252/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0638 - val_loss: 0.0631\n",
      "Epoch 253/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0635 - val_loss: 0.0650\n",
      "Epoch 254/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0634 - val_loss: 0.0645\n",
      "Epoch 255/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0637 - val_loss: 0.0643\n",
      "Epoch 256/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0635 - val_loss: 0.0627\n",
      "Epoch 257/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0633 - val_loss: 0.0637\n",
      "Epoch 258/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0636 - val_loss: 0.0628\n",
      "Epoch 259/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0633 - val_loss: 0.0641\n",
      "Epoch 260/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0634 - val_loss: 0.0625\n",
      "Epoch 261/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0635 - val_loss: 0.0630\n",
      "Epoch 262/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0634 - val_loss: 0.0629\n",
      "Epoch 263/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0632 - val_loss: 0.0609\n",
      "Epoch 264/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0631 - val_loss: 0.0633\n",
      "Epoch 265/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0630 - val_loss: 0.0623\n",
      "Epoch 266/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0632 - val_loss: 0.0612\n",
      "Epoch 267/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0632 - val_loss: 0.0637\n",
      "Epoch 268/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0630 - val_loss: 0.0660\n",
      "Epoch 269/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0631 - val_loss: 0.0639\n",
      "Epoch 270/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0633 - val_loss: 0.0622\n",
      "Epoch 271/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0629 - val_loss: 0.0630\n",
      "Epoch 272/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0630 - val_loss: 0.0623\n",
      "Epoch 273/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0629 - val_loss: 0.0651\n",
      "Epoch 274/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0630 - val_loss: 0.0648\n",
      "Epoch 275/400\n",
      "1055/1055 [==============================] - 5s 5ms/step - loss: 0.0628 - val_loss: 0.0617\n",
      "Epoch 276/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0631 - val_loss: 0.0618\n",
      "Epoch 277/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0629 - val_loss: 0.0613\n",
      "Epoch 278/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0630 - val_loss: 0.0625\n",
      "Epoch 279/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0629 - val_loss: 0.0633\n",
      "Epoch 280/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0629 - val_loss: 0.0655\n",
      "Epoch 281/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0634 - val_loss: 0.0636\n",
      "Epoch 282/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0630 - val_loss: 0.0615\n",
      "Epoch 283/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0629 - val_loss: 0.0616\n",
      "Epoch 284/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0628 - val_loss: 0.0669\n",
      "Epoch 285/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0629 - val_loss: 0.0614\n",
      "Epoch 286/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0632 - val_loss: 0.0618\n",
      "Epoch 287/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0626 - val_loss: 0.0650\n",
      "Epoch 288/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0633 - val_loss: 0.0698\n",
      "Epoch 289/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0626 - val_loss: 0.0615\n",
      "Epoch 290/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0626 - val_loss: 0.0620\n",
      "Epoch 291/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0628 - val_loss: 0.0619\n",
      "Epoch 292/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0625 - val_loss: 0.0667\n",
      "Epoch 293/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0630 - val_loss: 0.0649\n",
      "Epoch 294/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0627 - val_loss: 0.0607\n",
      "Epoch 295/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0626 - val_loss: 0.0651\n",
      "Epoch 296/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0626 - val_loss: 0.0619\n",
      "Epoch 297/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0626 - val_loss: 0.0667\n",
      "Epoch 298/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0628 - val_loss: 0.0628\n",
      "Epoch 299/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0622 - val_loss: 0.0641\n",
      "Epoch 300/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0626 - val_loss: 0.0612\n",
      "Epoch 301/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0623 - val_loss: 0.0617\n",
      "Epoch 302/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0623 - val_loss: 0.0628\n",
      "Epoch 303/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0625 - val_loss: 0.0608\n",
      "Epoch 304/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0621 - val_loss: 0.0633\n",
      "Epoch 305/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0628 - val_loss: 0.0621\n",
      "Epoch 306/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0622 - val_loss: 0.0616\n",
      "Epoch 307/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0625 - val_loss: 0.0619\n",
      "Epoch 308/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0622 - val_loss: 0.0612\n",
      "Epoch 309/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0622 - val_loss: 0.0646\n",
      "Epoch 310/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0622 - val_loss: 0.0606\n",
      "Epoch 311/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0621 - val_loss: 0.0622\n",
      "Epoch 312/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0622 - val_loss: 0.0602\n",
      "Epoch 313/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0620 - val_loss: 0.0600\n",
      "Epoch 314/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0620 - val_loss: 0.0605\n",
      "Epoch 315/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0621 - val_loss: 0.0626\n",
      "Epoch 316/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0618 - val_loss: 0.0640\n",
      "Epoch 317/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0619 - val_loss: 0.0663\n",
      "Epoch 318/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0618 - val_loss: 0.0606\n",
      "Epoch 319/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0618 - val_loss: 0.0609\n",
      "Epoch 320/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0617 - val_loss: 0.0596\n",
      "Epoch 321/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0618 - val_loss: 0.0602\n",
      "Epoch 322/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0619 - val_loss: 0.0596\n",
      "Epoch 323/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0617 - val_loss: 0.0627\n",
      "Epoch 324/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0616 - val_loss: 0.0599\n",
      "Epoch 325/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0618 - val_loss: 0.0614\n",
      "Epoch 326/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0617 - val_loss: 0.0615\n",
      "Epoch 327/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0615 - val_loss: 0.0630\n",
      "Epoch 328/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0613 - val_loss: 0.0604\n",
      "Epoch 329/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0617 - val_loss: 0.0600\n",
      "Epoch 330/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0616 - val_loss: 0.0624\n",
      "Epoch 331/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0614 - val_loss: 0.0626\n",
      "Epoch 332/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0615 - val_loss: 0.0682\n",
      "Epoch 333/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0615 - val_loss: 0.0604\n",
      "Epoch 334/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0614 - val_loss: 0.0597\n",
      "Epoch 335/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0614 - val_loss: 0.0607\n",
      "Epoch 336/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0610 - val_loss: 0.0615\n",
      "Epoch 337/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0613 - val_loss: 0.0660\n",
      "Epoch 338/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0614 - val_loss: 0.0632\n",
      "Epoch 339/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0612 - val_loss: 0.0595\n",
      "Epoch 340/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0612 - val_loss: 0.0605\n",
      "Epoch 341/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0612 - val_loss: 0.0600\n",
      "Epoch 342/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0609 - val_loss: 0.0596\n",
      "Epoch 343/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0610 - val_loss: 0.0617\n",
      "Epoch 344/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0612 - val_loss: 0.0612\n",
      "Epoch 345/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0611 - val_loss: 0.0606\n",
      "Epoch 346/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0607 - val_loss: 0.0589\n",
      "Epoch 347/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0614 - val_loss: 0.0601\n",
      "Epoch 348/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0611 - val_loss: 0.0617\n",
      "Epoch 349/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0608 - val_loss: 0.0609\n",
      "Epoch 350/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0607 - val_loss: 0.0634\n",
      "Epoch 351/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0606 - val_loss: 0.0614\n",
      "Epoch 352/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0610 - val_loss: 0.0601\n",
      "Epoch 353/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0610 - val_loss: 0.0613\n",
      "Epoch 354/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0608 - val_loss: 0.0593\n",
      "Epoch 355/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0609 - val_loss: 0.0603\n",
      "Epoch 356/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0607 - val_loss: 0.0640\n",
      "Epoch 357/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0605 - val_loss: 0.0616\n",
      "Epoch 358/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0608 - val_loss: 0.0623\n",
      "Epoch 359/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0605 - val_loss: 0.0596\n",
      "Epoch 360/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0608 - val_loss: 0.0616\n",
      "Epoch 361/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0608 - val_loss: 0.0605\n",
      "Epoch 362/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0605 - val_loss: 0.0603\n",
      "Epoch 363/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0605 - val_loss: 0.0592\n",
      "Epoch 364/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0605 - val_loss: 0.0595\n",
      "Epoch 365/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0607 - val_loss: 0.0590\n",
      "Epoch 366/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0603 - val_loss: 0.0615\n",
      "Epoch 367/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0606 - val_loss: 0.0590\n",
      "Epoch 368/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0607 - val_loss: 0.0593\n",
      "Epoch 369/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0604 - val_loss: 0.0605\n",
      "Epoch 370/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0610 - val_loss: 0.0613\n",
      "Epoch 371/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0605 - val_loss: 0.0604\n",
      "Epoch 372/400\n",
      "1055/1055 [==============================] - 6s 6ms/step - loss: 0.0607 - val_loss: 0.0611\n",
      "Epoch 373/400\n",
      "1055/1055 [==============================] - 5s 5ms/step - loss: 0.0605 - val_loss: 0.0594\n",
      "Epoch 374/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0605 - val_loss: 0.0613\n",
      "Epoch 375/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0605 - val_loss: 0.0605\n",
      "Epoch 376/400\n",
      "1055/1055 [==============================] - 4s 4ms/step - loss: 0.0605 - val_loss: 0.0605\n",
      "Epoch 377/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0601 - val_loss: 0.0615\n",
      "Epoch 378/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0604 - val_loss: 0.0615\n",
      "Epoch 379/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0605 - val_loss: 0.0600\n",
      "Epoch 380/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0603 - val_loss: 0.0653\n",
      "Epoch 381/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0604 - val_loss: 0.0590\n",
      "Epoch 382/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0603 - val_loss: 0.0588\n",
      "Epoch 383/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0602 - val_loss: 0.0582\n",
      "Epoch 384/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0600 - val_loss: 0.0607\n",
      "Epoch 385/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0601 - val_loss: 0.0625\n",
      "Epoch 386/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0604 - val_loss: 0.0607\n",
      "Epoch 387/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0601 - val_loss: 0.0601\n",
      "Epoch 388/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0600 - val_loss: 0.0627\n",
      "Epoch 389/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0599 - val_loss: 0.0607\n",
      "Epoch 390/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0602 - val_loss: 0.0597\n",
      "Epoch 391/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0598 - val_loss: 0.0629\n",
      "Epoch 392/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0600 - val_loss: 0.0586\n",
      "Epoch 393/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0598 - val_loss: 0.0583\n",
      "Epoch 394/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0598 - val_loss: 0.0605\n",
      "Epoch 395/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0601 - val_loss: 0.0590\n",
      "Epoch 396/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0599 - val_loss: 0.0591\n",
      "Epoch 397/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0599 - val_loss: 0.0577\n",
      "Epoch 398/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0603 - val_loss: 0.0613\n",
      "Epoch 399/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0598 - val_loss: 0.0661\n",
      "Epoch 400/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0601 - val_loss: 0.0605\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 9)                 189       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 18)                180       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 9)                 171       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 892\n",
      "Trainable params: 892\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.fit(x=X_train,y=y_train,\n",
    "          validation_data=(X_test,y_test),\n",
    "          batch_size=128,epochs=400)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19402c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.1825113728085573\n",
      "MSE: 0.06048555962023627\n",
      "RMSE: 0.24593812152701394\n",
      "VarScore: 0.7340551777356594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2ab6e8661f0>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "from sklearn import metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('VarScore:',metrics.explained_variance_score(y_test,y_pred))\n",
    "# Visualizing Our predictions\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.scatter(y_test,y_pred)\n",
    "# Perfect predictions\n",
    "plt.plot(y_test,y_test,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e526cdd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.560682</td>\n",
       "      <td>5.868451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.886104</td>\n",
       "      <td>5.959053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.700480</td>\n",
       "      <td>4.987914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.645447</td>\n",
       "      <td>5.475070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.356709</td>\n",
       "      <td>4.509443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.394449</td>\n",
       "      <td>4.530380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.545177</td>\n",
       "      <td>5.606577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.919981</td>\n",
       "      <td>4.994384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.298317</td>\n",
       "      <td>5.631250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.337538</td>\n",
       "      <td>5.424427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.208590</td>\n",
       "      <td>5.233903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.198497</td>\n",
       "      <td>5.265334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.068904</td>\n",
       "      <td>5.544894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.595120</td>\n",
       "      <td>4.790652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.181784</td>\n",
       "      <td>4.949882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.308268</td>\n",
       "      <td>5.291341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.176150</td>\n",
       "      <td>5.395319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.343805</td>\n",
       "      <td>4.511836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.056784</td>\n",
       "      <td>6.137824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.793014</td>\n",
       "      <td>5.357530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual  Predicted\n",
       "0   5.560682   5.868451\n",
       "1   5.886104   5.959053\n",
       "2   4.700480   4.987914\n",
       "3   5.645447   5.475070\n",
       "4   4.356709   4.509443\n",
       "5   4.394449   4.530380\n",
       "6   5.545177   5.606577\n",
       "7   4.919981   4.994384\n",
       "8   5.298317   5.631250\n",
       "9   5.337538   5.424427\n",
       "10  6.208590   5.233903\n",
       "11  5.198497   5.265334\n",
       "12  5.068904   5.544894\n",
       "13  4.595120   4.790652\n",
       "14  5.181784   4.949882\n",
       "15  5.308268   5.291341\n",
       "16  5.176150   5.395319\n",
       "17  4.343805   4.511836\n",
       "18  6.056784   6.137824\n",
       "19  5.793014   5.357530"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_2 = []\n",
    "for pred in y_pred:\n",
    "  y_pred_2.append(pred[0])\n",
    "\n",
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_2})\n",
    "df1 = df.head(20)\n",
    "df1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
