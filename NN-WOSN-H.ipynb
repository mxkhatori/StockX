{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc10fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from numpy import unique\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46bab103",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\49173\\Desktop\\Data\\stockxf2.csv\")\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4290272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>productId</th>\n",
       "      <th>orderDate</th>\n",
       "      <th>salePrice</th>\n",
       "      <th>shoeSize</th>\n",
       "      <th>brand</th>\n",
       "      <th>sneakerName</th>\n",
       "      <th>colorway</th>\n",
       "      <th>retailPrice</th>\n",
       "      <th>releaseDate</th>\n",
       "      <th>salesThisPeriod</th>\n",
       "      <th>hype</th>\n",
       "      <th>days</th>\n",
       "      <th>collaboration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>55a9ccfe-129a-438a-9989-b877c2a66279</td>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>300</td>\n",
       "      <td>9.5</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>Jordan 4 Retro Military Black</td>\n",
       "      <td>White/Black-Neutral Grey</td>\n",
       "      <td>210</td>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>2476</td>\n",
       "      <td>10743</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55a9ccfe-129a-438a-9989-b877c2a66279</td>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>300</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>Jordan 4 Retro Military Black</td>\n",
       "      <td>White/Black-Neutral Grey</td>\n",
       "      <td>210</td>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>2476</td>\n",
       "      <td>10743</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55a9ccfe-129a-438a-9989-b877c2a66279</td>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>382</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>Jordan 4 Retro Military Black</td>\n",
       "      <td>White/Black-Neutral Grey</td>\n",
       "      <td>210</td>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>2476</td>\n",
       "      <td>10743</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55a9ccfe-129a-438a-9989-b877c2a66279</td>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>304</td>\n",
       "      <td>8.0</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>Jordan 4 Retro Military Black</td>\n",
       "      <td>White/Black-Neutral Grey</td>\n",
       "      <td>210</td>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>2476</td>\n",
       "      <td>10743</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55a9ccfe-129a-438a-9989-b877c2a66279</td>\n",
       "      <td>2022-05-26</td>\n",
       "      <td>390</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>Jordan 4 Retro Military Black</td>\n",
       "      <td>White/Black-Neutral Grey</td>\n",
       "      <td>210</td>\n",
       "      <td>2022-05-21</td>\n",
       "      <td>2476</td>\n",
       "      <td>10743</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192880</th>\n",
       "      <td>3774e829-99f4-46d6-bb94-f9e19ad55ad3</td>\n",
       "      <td>2022-03-18</td>\n",
       "      <td>384</td>\n",
       "      <td>10.0</td>\n",
       "      <td>adidas</td>\n",
       "      <td>adidas Yeezy Boost 350 V2 Sesame</td>\n",
       "      <td>Sesame/Sesame/Sesame</td>\n",
       "      <td>220</td>\n",
       "      <td>2018-11-23</td>\n",
       "      <td>13</td>\n",
       "      <td>5366</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192881</th>\n",
       "      <td>3774e829-99f4-46d6-bb94-f9e19ad55ad3</td>\n",
       "      <td>2022-03-18</td>\n",
       "      <td>366</td>\n",
       "      <td>12.0</td>\n",
       "      <td>adidas</td>\n",
       "      <td>adidas Yeezy Boost 350 V2 Sesame</td>\n",
       "      <td>Sesame/Sesame/Sesame</td>\n",
       "      <td>220</td>\n",
       "      <td>2018-11-23</td>\n",
       "      <td>13</td>\n",
       "      <td>5366</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192882</th>\n",
       "      <td>3774e829-99f4-46d6-bb94-f9e19ad55ad3</td>\n",
       "      <td>2022-03-18</td>\n",
       "      <td>420</td>\n",
       "      <td>10.5</td>\n",
       "      <td>adidas</td>\n",
       "      <td>adidas Yeezy Boost 350 V2 Sesame</td>\n",
       "      <td>Sesame/Sesame/Sesame</td>\n",
       "      <td>220</td>\n",
       "      <td>2018-11-23</td>\n",
       "      <td>13</td>\n",
       "      <td>5366</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192883</th>\n",
       "      <td>3774e829-99f4-46d6-bb94-f9e19ad55ad3</td>\n",
       "      <td>2022-03-18</td>\n",
       "      <td>357</td>\n",
       "      <td>11.0</td>\n",
       "      <td>adidas</td>\n",
       "      <td>adidas Yeezy Boost 350 V2 Sesame</td>\n",
       "      <td>Sesame/Sesame/Sesame</td>\n",
       "      <td>220</td>\n",
       "      <td>2018-11-23</td>\n",
       "      <td>13</td>\n",
       "      <td>5366</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192884</th>\n",
       "      <td>3774e829-99f4-46d6-bb94-f9e19ad55ad3</td>\n",
       "      <td>2022-03-18</td>\n",
       "      <td>383</td>\n",
       "      <td>9.0</td>\n",
       "      <td>adidas</td>\n",
       "      <td>adidas Yeezy Boost 350 V2 Sesame</td>\n",
       "      <td>Sesame/Sesame/Sesame</td>\n",
       "      <td>220</td>\n",
       "      <td>2018-11-23</td>\n",
       "      <td>13</td>\n",
       "      <td>5366</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192885 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   productId   orderDate  salePrice  shoeSize  \\\n",
       "0       55a9ccfe-129a-438a-9989-b877c2a66279  2022-05-26        300       9.5   \n",
       "1       55a9ccfe-129a-438a-9989-b877c2a66279  2022-05-26        300       9.0   \n",
       "2       55a9ccfe-129a-438a-9989-b877c2a66279  2022-05-26        382      10.0   \n",
       "3       55a9ccfe-129a-438a-9989-b877c2a66279  2022-05-26        304       8.0   \n",
       "4       55a9ccfe-129a-438a-9989-b877c2a66279  2022-05-26        390      12.0   \n",
       "...                                      ...         ...        ...       ...   \n",
       "192880  3774e829-99f4-46d6-bb94-f9e19ad55ad3  2022-03-18        384      10.0   \n",
       "192881  3774e829-99f4-46d6-bb94-f9e19ad55ad3  2022-03-18        366      12.0   \n",
       "192882  3774e829-99f4-46d6-bb94-f9e19ad55ad3  2022-03-18        420      10.5   \n",
       "192883  3774e829-99f4-46d6-bb94-f9e19ad55ad3  2022-03-18        357      11.0   \n",
       "192884  3774e829-99f4-46d6-bb94-f9e19ad55ad3  2022-03-18        383       9.0   \n",
       "\n",
       "         brand                       sneakerName                  colorway  \\\n",
       "0       Jordan     Jordan 4 Retro Military Black  White/Black-Neutral Grey   \n",
       "1       Jordan     Jordan 4 Retro Military Black  White/Black-Neutral Grey   \n",
       "2       Jordan     Jordan 4 Retro Military Black  White/Black-Neutral Grey   \n",
       "3       Jordan     Jordan 4 Retro Military Black  White/Black-Neutral Grey   \n",
       "4       Jordan     Jordan 4 Retro Military Black  White/Black-Neutral Grey   \n",
       "...        ...                               ...                       ...   \n",
       "192880  adidas  adidas Yeezy Boost 350 V2 Sesame      Sesame/Sesame/Sesame   \n",
       "192881  adidas  adidas Yeezy Boost 350 V2 Sesame      Sesame/Sesame/Sesame   \n",
       "192882  adidas  adidas Yeezy Boost 350 V2 Sesame      Sesame/Sesame/Sesame   \n",
       "192883  adidas  adidas Yeezy Boost 350 V2 Sesame      Sesame/Sesame/Sesame   \n",
       "192884  adidas  adidas Yeezy Boost 350 V2 Sesame      Sesame/Sesame/Sesame   \n",
       "\n",
       "        retailPrice releaseDate  salesThisPeriod   hype  days  collaboration  \n",
       "0               210  2022-05-21             2476  10743     5              1  \n",
       "1               210  2022-05-21             2476  10743     5              1  \n",
       "2               210  2022-05-21             2476  10743     5              1  \n",
       "3               210  2022-05-21             2476  10743     5              1  \n",
       "4               210  2022-05-21             2476  10743     5              1  \n",
       "...             ...         ...              ...    ...   ...            ...  \n",
       "192880          220  2018-11-23               13   5366  1211              1  \n",
       "192881          220  2018-11-23               13   5366  1211              1  \n",
       "192882          220  2018-11-23               13   5366  1211              1  \n",
       "192883          220  2018-11-23               13   5366  1211              1  \n",
       "192884          220  2018-11-23               13   5366  1211              1  \n",
       "\n",
       "[192885 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56742a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['log_hype'] = np.log(df['hype']+1)\n",
    "df['log_resalePrice'] = np.log(df['salePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a609b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['productId', \n",
    "              'orderDate',\n",
    "              'sneakerName',\n",
    "              'colorway', \n",
    "              'releaseDate', \n",
    "              \"hype\", \n",
    "              'salePrice'],\n",
    "              axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44273c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shoeSize</th>\n",
       "      <th>brand</th>\n",
       "      <th>retailPrice</th>\n",
       "      <th>salesThisPeriod</th>\n",
       "      <th>days</th>\n",
       "      <th>collaboration</th>\n",
       "      <th>log_hype</th>\n",
       "      <th>log_resalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.5</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9.282103</td>\n",
       "      <td>5.703782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9.282103</td>\n",
       "      <td>5.703782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9.282103</td>\n",
       "      <td>5.945421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.0</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9.282103</td>\n",
       "      <td>5.717028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.0</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9.282103</td>\n",
       "      <td>5.966147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192880</th>\n",
       "      <td>10.0</td>\n",
       "      <td>adidas</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>8.588024</td>\n",
       "      <td>5.950643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192881</th>\n",
       "      <td>12.0</td>\n",
       "      <td>adidas</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>8.588024</td>\n",
       "      <td>5.902633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192882</th>\n",
       "      <td>10.5</td>\n",
       "      <td>adidas</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>8.588024</td>\n",
       "      <td>6.040255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192883</th>\n",
       "      <td>11.0</td>\n",
       "      <td>adidas</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>8.588024</td>\n",
       "      <td>5.877736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192884</th>\n",
       "      <td>9.0</td>\n",
       "      <td>adidas</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>8.588024</td>\n",
       "      <td>5.948035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192885 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        shoeSize   brand  retailPrice  salesThisPeriod  days  collaboration  \\\n",
       "0            9.5  Jordan          210             2476     5              1   \n",
       "1            9.0  Jordan          210             2476     5              1   \n",
       "2           10.0  Jordan          210             2476     5              1   \n",
       "3            8.0  Jordan          210             2476     5              1   \n",
       "4           12.0  Jordan          210             2476     5              1   \n",
       "...          ...     ...          ...              ...   ...            ...   \n",
       "192880      10.0  adidas          220               13  1211              1   \n",
       "192881      12.0  adidas          220               13  1211              1   \n",
       "192882      10.5  adidas          220               13  1211              1   \n",
       "192883      11.0  adidas          220               13  1211              1   \n",
       "192884       9.0  adidas          220               13  1211              1   \n",
       "\n",
       "        log_hype  log_resalePrice  \n",
       "0       9.282103         5.703782  \n",
       "1       9.282103         5.703782  \n",
       "2       9.282103         5.945421  \n",
       "3       9.282103         5.717028  \n",
       "4       9.282103         5.966147  \n",
       "...          ...              ...  \n",
       "192880  8.588024         5.950643  \n",
       "192881  8.588024         5.902633  \n",
       "192882  8.588024         6.040255  \n",
       "192883  8.588024         5.877736  \n",
       "192884  8.588024         5.948035  \n",
       "\n",
       "[192885 rows x 8 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ab43868",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.get_dummies(df[\"brand\"], prefix='brand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8858088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['brand'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bf4d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 = pd.get_dummies(df[\"sneakerName\"], prefix='SN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49e5f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.drop(['sneakerName'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4273f4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shoeSize</th>\n",
       "      <th>retailPrice</th>\n",
       "      <th>salesThisPeriod</th>\n",
       "      <th>days</th>\n",
       "      <th>collaboration</th>\n",
       "      <th>log_hype</th>\n",
       "      <th>log_resalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.5</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9.282103</td>\n",
       "      <td>5.703782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9.282103</td>\n",
       "      <td>5.703782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9.282103</td>\n",
       "      <td>5.945421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.0</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9.282103</td>\n",
       "      <td>5.717028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.0</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9.282103</td>\n",
       "      <td>5.966147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192880</th>\n",
       "      <td>10.0</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>8.588024</td>\n",
       "      <td>5.950643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192881</th>\n",
       "      <td>12.0</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>8.588024</td>\n",
       "      <td>5.902633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192882</th>\n",
       "      <td>10.5</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>8.588024</td>\n",
       "      <td>6.040255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192883</th>\n",
       "      <td>11.0</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>8.588024</td>\n",
       "      <td>5.877736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192884</th>\n",
       "      <td>9.0</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>8.588024</td>\n",
       "      <td>5.948035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192885 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        shoeSize  retailPrice  salesThisPeriod  days  collaboration  log_hype  \\\n",
       "0            9.5          210             2476     5              1  9.282103   \n",
       "1            9.0          210             2476     5              1  9.282103   \n",
       "2           10.0          210             2476     5              1  9.282103   \n",
       "3            8.0          210             2476     5              1  9.282103   \n",
       "4           12.0          210             2476     5              1  9.282103   \n",
       "...          ...          ...              ...   ...            ...       ...   \n",
       "192880      10.0          220               13  1211              1  8.588024   \n",
       "192881      12.0          220               13  1211              1  8.588024   \n",
       "192882      10.5          220               13  1211              1  8.588024   \n",
       "192883      11.0          220               13  1211              1  8.588024   \n",
       "192884       9.0          220               13  1211              1  8.588024   \n",
       "\n",
       "        log_resalePrice  \n",
       "0              5.703782  \n",
       "1              5.703782  \n",
       "2              5.945421  \n",
       "3              5.717028  \n",
       "4              5.966147  \n",
       "...                 ...  \n",
       "192880         5.950643  \n",
       "192881         5.902633  \n",
       "192882         6.040255  \n",
       "192883         5.877736  \n",
       "192884         5.948035  \n",
       "\n",
       "[192885 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0296e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6118a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.join(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c61c5eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shoeSize</th>\n",
       "      <th>retailPrice</th>\n",
       "      <th>salesThisPeriod</th>\n",
       "      <th>days</th>\n",
       "      <th>collaboration</th>\n",
       "      <th>log_hype</th>\n",
       "      <th>log_resalePrice</th>\n",
       "      <th>brand_Alexander McQueen</th>\n",
       "      <th>brand_BAPE</th>\n",
       "      <th>brand_Common Projects</th>\n",
       "      <th>...</th>\n",
       "      <th>brand_Jordan</th>\n",
       "      <th>brand_MSCHF</th>\n",
       "      <th>brand_New Balance</th>\n",
       "      <th>brand_Nike</th>\n",
       "      <th>brand_Puma</th>\n",
       "      <th>brand_Reebok</th>\n",
       "      <th>brand_Salomon</th>\n",
       "      <th>brand_Under Armour</th>\n",
       "      <th>brand_Vans</th>\n",
       "      <th>brand_adidas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.5</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9.282103</td>\n",
       "      <td>5.703782</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.0</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9.282103</td>\n",
       "      <td>5.703782</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9.282103</td>\n",
       "      <td>5.945421</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.0</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9.282103</td>\n",
       "      <td>5.717028</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.0</td>\n",
       "      <td>210</td>\n",
       "      <td>2476</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9.282103</td>\n",
       "      <td>5.966147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192880</th>\n",
       "      <td>10.0</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>8.588024</td>\n",
       "      <td>5.950643</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192881</th>\n",
       "      <td>12.0</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>8.588024</td>\n",
       "      <td>5.902633</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192882</th>\n",
       "      <td>10.5</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>8.588024</td>\n",
       "      <td>6.040255</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192883</th>\n",
       "      <td>11.0</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>8.588024</td>\n",
       "      <td>5.877736</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192884</th>\n",
       "      <td>9.0</td>\n",
       "      <td>220</td>\n",
       "      <td>13</td>\n",
       "      <td>1211</td>\n",
       "      <td>1</td>\n",
       "      <td>8.588024</td>\n",
       "      <td>5.948035</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192885 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        shoeSize  retailPrice  salesThisPeriod  days  collaboration  log_hype  \\\n",
       "0            9.5          210             2476     5              1  9.282103   \n",
       "1            9.0          210             2476     5              1  9.282103   \n",
       "2           10.0          210             2476     5              1  9.282103   \n",
       "3            8.0          210             2476     5              1  9.282103   \n",
       "4           12.0          210             2476     5              1  9.282103   \n",
       "...          ...          ...              ...   ...            ...       ...   \n",
       "192880      10.0          220               13  1211              1  8.588024   \n",
       "192881      12.0          220               13  1211              1  8.588024   \n",
       "192882      10.5          220               13  1211              1  8.588024   \n",
       "192883      11.0          220               13  1211              1  8.588024   \n",
       "192884       9.0          220               13  1211              1  8.588024   \n",
       "\n",
       "        log_resalePrice  brand_Alexander McQueen  brand_BAPE  \\\n",
       "0              5.703782                        0           0   \n",
       "1              5.703782                        0           0   \n",
       "2              5.945421                        0           0   \n",
       "3              5.717028                        0           0   \n",
       "4              5.966147                        0           0   \n",
       "...                 ...                      ...         ...   \n",
       "192880         5.950643                        0           0   \n",
       "192881         5.902633                        0           0   \n",
       "192882         6.040255                        0           0   \n",
       "192883         5.877736                        0           0   \n",
       "192884         5.948035                        0           0   \n",
       "\n",
       "        brand_Common Projects  ...  brand_Jordan  brand_MSCHF  \\\n",
       "0                           0  ...             1            0   \n",
       "1                           0  ...             1            0   \n",
       "2                           0  ...             1            0   \n",
       "3                           0  ...             1            0   \n",
       "4                           0  ...             1            0   \n",
       "...                       ...  ...           ...          ...   \n",
       "192880                      0  ...             0            0   \n",
       "192881                      0  ...             0            0   \n",
       "192882                      0  ...             0            0   \n",
       "192883                      0  ...             0            0   \n",
       "192884                      0  ...             0            0   \n",
       "\n",
       "        brand_New Balance  brand_Nike  brand_Puma  brand_Reebok  \\\n",
       "0                       0           0           0             0   \n",
       "1                       0           0           0             0   \n",
       "2                       0           0           0             0   \n",
       "3                       0           0           0             0   \n",
       "4                       0           0           0             0   \n",
       "...                   ...         ...         ...           ...   \n",
       "192880                  0           0           0             0   \n",
       "192881                  0           0           0             0   \n",
       "192882                  0           0           0             0   \n",
       "192883                  0           0           0             0   \n",
       "192884                  0           0           0             0   \n",
       "\n",
       "        brand_Salomon  brand_Under Armour  brand_Vans  brand_adidas  \n",
       "0                   0                   0           0             0  \n",
       "1                   0                   0           0             0  \n",
       "2                   0                   0           0             0  \n",
       "3                   0                   0           0             0  \n",
       "4                   0                   0           0             0  \n",
       "...               ...                 ...         ...           ...  \n",
       "192880              0                   0           0             1  \n",
       "192881              0                   0           0             1  \n",
       "192882              0                   0           0             1  \n",
       "192883              0                   0           0             1  \n",
       "192884              0                   0           0             1  \n",
       "\n",
       "[192885 rows x 22 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82376efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['constant', 'shoeSize', 'retailPrice', 'salesThisPeriod', 'days', 'collaboration', 'log_hype', 'brand_Alexander McQueen', 'brand_BAPE', 'brand_Common Projects', 'brand_Converse', 'brand_Crocs', 'brand_Jordan', 'brand_MSCHF', 'brand_New Balance', 'brand_Nike', 'brand_Puma', 'brand_Reebok', 'brand_Salomon', 'brand_Under Armour', 'brand_Vans', 'brand_adidas']\n"
     ]
    }
   ],
   "source": [
    "dfnames = df.copy()\n",
    "dfnames = dfnames.drop(['log_resalePrice'], axis = 1)\n",
    "\n",
    "columns = dfnames.columns.tolist()\n",
    "columns.insert(0, 'constant')\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7ce940f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('log_resalePrice',axis =1).values\n",
    "y = df['log_resalePrice'].values\n",
    "\n",
    "#splitting Train and Test \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ca1e7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\49173\\AppData\\Local\\Temp\\ipykernel_21188\\2855883272.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_train = s_scaler.fit_transform(X_train.astype(np.float))\n",
      "C:\\Users\\49173\\AppData\\Local\\Temp\\ipykernel_21188\\2855883272.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_test = s_scaler.transform(X_test.astype(np.float))\n"
     ]
    }
   ],
   "source": [
    "s_scaler = StandardScaler()\n",
    "X_train = s_scaler.fit_transform(X_train.astype(np.float))\n",
    "X_test = s_scaler.transform(X_test.astype(np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1fe869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Neural Network Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44b1f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# having 9 neurons is based on the number of available features\n",
    "model = Sequential()\n",
    "model.add(Dense(9,activation='relu'))\n",
    "model.add(Dense(18,activation='relu'))\n",
    "model.add(Dense(18,activation='relu'))\n",
    "model.add(Dense(9,activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='Adam',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e02c26bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "1055/1055 [==============================] - 7s 3ms/step - loss: 1.9613 - val_loss: 0.1290\n",
      "Epoch 2/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.1165 - val_loss: 0.1112\n",
      "Epoch 3/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.1066 - val_loss: 0.1072\n",
      "Epoch 4/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.1018 - val_loss: 0.1025\n",
      "Epoch 5/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0988 - val_loss: 0.0991\n",
      "Epoch 6/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0971 - val_loss: 0.0967\n",
      "Epoch 7/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0953 - val_loss: 0.0948\n",
      "Epoch 8/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0931 - val_loss: 0.0926\n",
      "Epoch 9/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0921 - val_loss: 0.0927\n",
      "Epoch 10/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0903 - val_loss: 0.0920\n",
      "Epoch 11/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0893 - val_loss: 0.0901\n",
      "Epoch 12/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0883 - val_loss: 0.0904\n",
      "Epoch 13/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0868 - val_loss: 0.0866\n",
      "Epoch 14/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0859 - val_loss: 0.0867\n",
      "Epoch 15/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0850 - val_loss: 0.0848\n",
      "Epoch 16/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0844 - val_loss: 0.0842\n",
      "Epoch 17/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0841 - val_loss: 0.0844\n",
      "Epoch 18/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0835 - val_loss: 0.0837\n",
      "Epoch 19/400\n",
      "1055/1055 [==============================] - 5s 4ms/step - loss: 0.0830 - val_loss: 0.0841\n",
      "Epoch 20/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0826 - val_loss: 0.0857\n",
      "Epoch 21/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0818 - val_loss: 0.0830\n",
      "Epoch 22/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0816 - val_loss: 0.0819\n",
      "Epoch 23/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0813 - val_loss: 0.0846\n",
      "Epoch 24/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0803 - val_loss: 0.0810\n",
      "Epoch 25/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0801 - val_loss: 0.0815\n",
      "Epoch 26/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0796 - val_loss: 0.0811\n",
      "Epoch 27/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0790 - val_loss: 0.0845\n",
      "Epoch 28/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0789 - val_loss: 0.0823\n",
      "Epoch 29/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0779 - val_loss: 0.0803\n",
      "Epoch 30/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0782 - val_loss: 0.0788\n",
      "Epoch 31/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0778 - val_loss: 0.0786\n",
      "Epoch 32/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0777 - val_loss: 0.0797\n",
      "Epoch 33/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0771 - val_loss: 0.0797\n",
      "Epoch 34/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0769 - val_loss: 0.0799\n",
      "Epoch 35/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0770 - val_loss: 0.0765\n",
      "Epoch 36/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0770 - val_loss: 0.0769\n",
      "Epoch 37/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0764 - val_loss: 0.0808\n",
      "Epoch 38/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0760 - val_loss: 0.0777\n",
      "Epoch 39/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0760 - val_loss: 0.0872\n",
      "Epoch 40/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0755 - val_loss: 0.0763\n",
      "Epoch 41/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0755 - val_loss: 0.0788\n",
      "Epoch 42/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0752 - val_loss: 0.0766\n",
      "Epoch 43/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0752 - val_loss: 0.0787\n",
      "Epoch 44/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0750 - val_loss: 0.0764\n",
      "Epoch 45/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0743 - val_loss: 0.0748\n",
      "Epoch 46/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0742 - val_loss: 0.0775\n",
      "Epoch 47/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0740 - val_loss: 0.0783\n",
      "Epoch 48/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0737 - val_loss: 0.0745\n",
      "Epoch 49/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0732 - val_loss: 0.0755\n",
      "Epoch 50/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0736 - val_loss: 0.0749\n",
      "Epoch 51/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0731 - val_loss: 0.0730\n",
      "Epoch 52/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0727 - val_loss: 0.0793\n",
      "Epoch 53/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0725 - val_loss: 0.0746\n",
      "Epoch 54/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0726 - val_loss: 0.0737\n",
      "Epoch 55/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0721 - val_loss: 0.0741\n",
      "Epoch 56/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0720 - val_loss: 0.0742\n",
      "Epoch 57/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0719 - val_loss: 0.0747\n",
      "Epoch 58/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0718 - val_loss: 0.0726\n",
      "Epoch 59/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0715 - val_loss: 0.0729\n",
      "Epoch 60/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0709 - val_loss: 0.0746\n",
      "Epoch 61/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0708 - val_loss: 0.0720\n",
      "Epoch 62/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0704 - val_loss: 0.0718\n",
      "Epoch 63/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0704 - val_loss: 0.0724\n",
      "Epoch 64/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0702 - val_loss: 0.0707\n",
      "Epoch 65/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0699 - val_loss: 0.0700\n",
      "Epoch 66/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0695 - val_loss: 0.0709\n",
      "Epoch 67/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0699 - val_loss: 0.0714\n",
      "Epoch 68/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0693 - val_loss: 0.0701\n",
      "Epoch 69/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0693 - val_loss: 0.0709\n",
      "Epoch 70/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0690 - val_loss: 0.0757\n",
      "Epoch 71/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0690 - val_loss: 0.0692\n",
      "Epoch 72/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0688 - val_loss: 0.0706\n",
      "Epoch 73/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0685 - val_loss: 0.0683\n",
      "Epoch 74/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0684 - val_loss: 0.0690\n",
      "Epoch 75/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0685 - val_loss: 0.0700\n",
      "Epoch 76/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0682 - val_loss: 0.0678\n",
      "Epoch 77/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0680 - val_loss: 0.0687\n",
      "Epoch 78/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0676 - val_loss: 0.0673\n",
      "Epoch 79/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0677 - val_loss: 0.0673\n",
      "Epoch 80/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0674 - val_loss: 0.0701\n",
      "Epoch 81/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0674 - val_loss: 0.0688\n",
      "Epoch 82/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0674 - val_loss: 0.0681\n",
      "Epoch 83/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0668 - val_loss: 0.0686\n",
      "Epoch 84/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0669 - val_loss: 0.0717\n",
      "Epoch 85/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0670 - val_loss: 0.0673\n",
      "Epoch 86/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0665 - val_loss: 0.0683\n",
      "Epoch 87/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0663 - val_loss: 0.0688\n",
      "Epoch 88/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0664 - val_loss: 0.0671\n",
      "Epoch 89/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0664 - val_loss: 0.0666\n",
      "Epoch 90/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0662 - val_loss: 0.0705\n",
      "Epoch 91/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0657 - val_loss: 0.0704\n",
      "Epoch 92/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0661 - val_loss: 0.0680\n",
      "Epoch 93/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0655 - val_loss: 0.0666\n",
      "Epoch 94/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0655 - val_loss: 0.0672\n",
      "Epoch 95/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0654 - val_loss: 0.0694\n",
      "Epoch 96/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0653 - val_loss: 0.0652\n",
      "Epoch 97/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0649 - val_loss: 0.0671\n",
      "Epoch 98/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0647 - val_loss: 0.0649\n",
      "Epoch 99/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0650 - val_loss: 0.0694\n",
      "Epoch 100/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0647 - val_loss: 0.0662\n",
      "Epoch 101/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0647 - val_loss: 0.0688\n",
      "Epoch 102/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0646 - val_loss: 0.0648\n",
      "Epoch 103/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0645 - val_loss: 0.0680\n",
      "Epoch 104/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0645 - val_loss: 0.0699\n",
      "Epoch 105/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0640 - val_loss: 0.0655\n",
      "Epoch 106/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0641 - val_loss: 0.0635\n",
      "Epoch 107/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0640 - val_loss: 0.0729\n",
      "Epoch 108/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0641 - val_loss: 0.0658\n",
      "Epoch 109/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0639 - val_loss: 0.0678\n",
      "Epoch 110/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0639 - val_loss: 0.0627\n",
      "Epoch 111/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0636 - val_loss: 0.0647\n",
      "Epoch 112/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0634 - val_loss: 0.0641\n",
      "Epoch 113/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0637 - val_loss: 0.0643\n",
      "Epoch 114/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0635 - val_loss: 0.0675\n",
      "Epoch 115/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0634 - val_loss: 0.0631\n",
      "Epoch 116/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0633 - val_loss: 0.0628\n",
      "Epoch 117/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0631 - val_loss: 0.0681\n",
      "Epoch 118/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0635 - val_loss: 0.0630\n",
      "Epoch 119/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0632 - val_loss: 0.0633\n",
      "Epoch 120/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0630 - val_loss: 0.0657\n",
      "Epoch 121/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0625 - val_loss: 0.0630\n",
      "Epoch 122/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0628 - val_loss: 0.0637\n",
      "Epoch 123/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0628 - val_loss: 0.0630\n",
      "Epoch 124/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0629 - val_loss: 0.0656\n",
      "Epoch 125/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0625 - val_loss: 0.0632\n",
      "Epoch 126/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0629 - val_loss: 0.0639\n",
      "Epoch 127/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0624 - val_loss: 0.0616\n",
      "Epoch 128/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0624 - val_loss: 0.0630\n",
      "Epoch 129/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0621 - val_loss: 0.0613\n",
      "Epoch 130/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0625 - val_loss: 0.0619\n",
      "Epoch 131/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0623 - val_loss: 0.0627\n",
      "Epoch 132/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0623 - val_loss: 0.0619\n",
      "Epoch 133/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0621 - val_loss: 0.0634\n",
      "Epoch 134/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0620 - val_loss: 0.0641\n",
      "Epoch 135/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0621 - val_loss: 0.0625\n",
      "Epoch 136/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0624 - val_loss: 0.0685\n",
      "Epoch 137/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0623 - val_loss: 0.0622\n",
      "Epoch 138/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0623 - val_loss: 0.0620\n",
      "Epoch 139/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0618 - val_loss: 0.0623\n",
      "Epoch 140/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0616 - val_loss: 0.0639\n",
      "Epoch 141/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0618 - val_loss: 0.0640\n",
      "Epoch 142/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0617 - val_loss: 0.0633\n",
      "Epoch 143/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0616 - val_loss: 0.0622\n",
      "Epoch 144/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0617 - val_loss: 0.0614\n",
      "Epoch 145/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0616 - val_loss: 0.0610\n",
      "Epoch 146/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0611 - val_loss: 0.0633\n",
      "Epoch 147/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0612 - val_loss: 0.0608\n",
      "Epoch 148/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0613 - val_loss: 0.0605\n",
      "Epoch 149/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0611 - val_loss: 0.0603\n",
      "Epoch 150/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0610 - val_loss: 0.0608\n",
      "Epoch 151/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0610 - val_loss: 0.0627\n",
      "Epoch 152/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0608 - val_loss: 0.0617\n",
      "Epoch 153/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0609 - val_loss: 0.0626\n",
      "Epoch 154/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0606 - val_loss: 0.0602\n",
      "Epoch 155/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0607 - val_loss: 0.0607\n",
      "Epoch 156/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0604 - val_loss: 0.0623\n",
      "Epoch 157/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0606 - val_loss: 0.0614\n",
      "Epoch 158/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0605 - val_loss: 0.0627\n",
      "Epoch 159/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0603 - val_loss: 0.0602\n",
      "Epoch 160/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0604 - val_loss: 0.0615\n",
      "Epoch 161/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0602 - val_loss: 0.0594\n",
      "Epoch 162/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0600 - val_loss: 0.0643\n",
      "Epoch 163/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0601 - val_loss: 0.0601\n",
      "Epoch 164/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0603 - val_loss: 0.0627\n",
      "Epoch 165/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0597 - val_loss: 0.0608\n",
      "Epoch 166/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0600 - val_loss: 0.0590\n",
      "Epoch 167/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0601 - val_loss: 0.0618\n",
      "Epoch 168/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0596 - val_loss: 0.0610\n",
      "Epoch 169/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0599 - val_loss: 0.0592\n",
      "Epoch 170/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0599 - val_loss: 0.0609\n",
      "Epoch 171/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0598 - val_loss: 0.0624\n",
      "Epoch 172/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0596 - val_loss: 0.0599\n",
      "Epoch 173/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0600 - val_loss: 0.0600\n",
      "Epoch 174/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0597 - val_loss: 0.0598\n",
      "Epoch 175/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0593 - val_loss: 0.0611\n",
      "Epoch 176/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0597 - val_loss: 0.0593\n",
      "Epoch 177/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0593 - val_loss: 0.0592\n",
      "Epoch 178/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0595 - val_loss: 0.0652\n",
      "Epoch 179/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0592 - val_loss: 0.0589\n",
      "Epoch 180/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0593 - val_loss: 0.0596\n",
      "Epoch 181/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0593 - val_loss: 0.0596\n",
      "Epoch 182/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0592 - val_loss: 0.0614\n",
      "Epoch 183/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0594 - val_loss: 0.0591\n",
      "Epoch 184/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0589 - val_loss: 0.0608\n",
      "Epoch 185/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0593 - val_loss: 0.0597\n",
      "Epoch 186/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0591 - val_loss: 0.0620\n",
      "Epoch 187/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0591 - val_loss: 0.0579\n",
      "Epoch 188/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0588 - val_loss: 0.0595\n",
      "Epoch 189/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0586 - val_loss: 0.0639\n",
      "Epoch 190/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0588 - val_loss: 0.0607\n",
      "Epoch 191/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0590 - val_loss: 0.0602\n",
      "Epoch 192/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0589 - val_loss: 0.0599\n",
      "Epoch 193/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0586 - val_loss: 0.0582\n",
      "Epoch 194/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0587 - val_loss: 0.0617\n",
      "Epoch 195/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0586 - val_loss: 0.0584\n",
      "Epoch 196/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0584 - val_loss: 0.0583\n",
      "Epoch 197/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0588 - val_loss: 0.0590\n",
      "Epoch 198/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0583 - val_loss: 0.0581\n",
      "Epoch 199/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0583 - val_loss: 0.0624\n",
      "Epoch 200/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0586 - val_loss: 0.0572\n",
      "Epoch 201/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0582 - val_loss: 0.0576\n",
      "Epoch 202/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0583 - val_loss: 0.0574\n",
      "Epoch 203/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0582 - val_loss: 0.0621\n",
      "Epoch 204/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0585 - val_loss: 0.0603\n",
      "Epoch 205/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0581 - val_loss: 0.0624\n",
      "Epoch 206/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0583 - val_loss: 0.0576\n",
      "Epoch 207/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0578 - val_loss: 0.0601\n",
      "Epoch 208/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0578 - val_loss: 0.0607\n",
      "Epoch 209/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0578 - val_loss: 0.0638\n",
      "Epoch 210/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0577 - val_loss: 0.0581\n",
      "Epoch 211/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0578 - val_loss: 0.0572\n",
      "Epoch 212/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0579 - val_loss: 0.0588\n",
      "Epoch 213/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0575 - val_loss: 0.0583\n",
      "Epoch 214/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0579 - val_loss: 0.0584\n",
      "Epoch 215/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0578 - val_loss: 0.0593\n",
      "Epoch 216/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0573 - val_loss: 0.0572\n",
      "Epoch 217/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0576 - val_loss: 0.0567\n",
      "Epoch 218/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0577 - val_loss: 0.0601\n",
      "Epoch 219/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0574 - val_loss: 0.0571\n",
      "Epoch 220/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0570 - val_loss: 0.0607\n",
      "Epoch 221/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0572 - val_loss: 0.0576\n",
      "Epoch 222/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0572 - val_loss: 0.0567\n",
      "Epoch 223/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0575 - val_loss: 0.0580\n",
      "Epoch 224/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0568 - val_loss: 0.0558\n",
      "Epoch 225/400\n",
      "1055/1055 [==============================] - 4s 3ms/step - loss: 0.0571 - val_loss: 0.0579\n",
      "Epoch 226/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0570 - val_loss: 0.0563\n",
      "Epoch 227/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0570 - val_loss: 0.0560\n",
      "Epoch 228/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0570 - val_loss: 0.0581\n",
      "Epoch 229/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0571 - val_loss: 0.0590\n",
      "Epoch 230/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0575 - val_loss: 0.0622\n",
      "Epoch 231/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0567 - val_loss: 0.0584\n",
      "Epoch 232/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0570 - val_loss: 0.0573\n",
      "Epoch 233/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0569 - val_loss: 0.0571\n",
      "Epoch 234/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0568 - val_loss: 0.0582\n",
      "Epoch 235/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0569 - val_loss: 0.0573\n",
      "Epoch 236/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0569 - val_loss: 0.0618\n",
      "Epoch 237/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0567 - val_loss: 0.0557\n",
      "Epoch 238/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0569 - val_loss: 0.0553\n",
      "Epoch 239/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0564 - val_loss: 0.0608\n",
      "Epoch 240/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0566 - val_loss: 0.0553\n",
      "Epoch 241/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0562 - val_loss: 0.0580\n",
      "Epoch 242/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0566 - val_loss: 0.0566\n",
      "Epoch 243/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0563 - val_loss: 0.0570\n",
      "Epoch 244/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0565 - val_loss: 0.0594\n",
      "Epoch 245/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0565 - val_loss: 0.0552\n",
      "Epoch 246/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0568 - val_loss: 0.0573\n",
      "Epoch 247/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0561 - val_loss: 0.0563\n",
      "Epoch 248/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0569 - val_loss: 0.0567\n",
      "Epoch 249/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0562 - val_loss: 0.0576\n",
      "Epoch 250/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0563 - val_loss: 0.0561\n",
      "Epoch 251/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0561 - val_loss: 0.0569\n",
      "Epoch 252/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0559 - val_loss: 0.0669\n",
      "Epoch 253/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0561 - val_loss: 0.0581\n",
      "Epoch 254/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0560 - val_loss: 0.0556\n",
      "Epoch 255/400\n",
      "1055/1055 [==============================] - 2s 2ms/step - loss: 0.0566 - val_loss: 0.0595\n",
      "Epoch 256/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0563 - val_loss: 0.0580\n",
      "Epoch 257/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0557 - val_loss: 0.0569\n",
      "Epoch 258/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0562 - val_loss: 0.0539\n",
      "Epoch 259/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0558 - val_loss: 0.0551\n",
      "Epoch 260/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0560 - val_loss: 0.0540\n",
      "Epoch 261/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0559 - val_loss: 0.0552\n",
      "Epoch 262/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0559 - val_loss: 0.0558\n",
      "Epoch 263/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0555 - val_loss: 0.0626\n",
      "Epoch 264/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0558 - val_loss: 0.0562\n",
      "Epoch 265/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0553 - val_loss: 0.0545\n",
      "Epoch 266/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0555 - val_loss: 0.0577\n",
      "Epoch 267/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0556 - val_loss: 0.0562\n",
      "Epoch 268/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0556 - val_loss: 0.0570\n",
      "Epoch 269/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0561 - val_loss: 0.0549\n",
      "Epoch 270/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0555 - val_loss: 0.0549\n",
      "Epoch 271/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0552 - val_loss: 0.0612\n",
      "Epoch 272/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0557 - val_loss: 0.0546\n",
      "Epoch 273/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0551 - val_loss: 0.0563\n",
      "Epoch 274/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0555 - val_loss: 0.0608\n",
      "Epoch 275/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0553 - val_loss: 0.0548\n",
      "Epoch 276/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0556 - val_loss: 0.0564\n",
      "Epoch 277/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0553 - val_loss: 0.0575\n",
      "Epoch 278/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0552 - val_loss: 0.0569\n",
      "Epoch 279/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0552 - val_loss: 0.0575\n",
      "Epoch 280/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0555 - val_loss: 0.0553\n",
      "Epoch 281/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0554 - val_loss: 0.0575\n",
      "Epoch 282/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0549 - val_loss: 0.0542\n",
      "Epoch 283/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0551 - val_loss: 0.0555\n",
      "Epoch 284/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0548 - val_loss: 0.0569\n",
      "Epoch 285/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0551 - val_loss: 0.0543\n",
      "Epoch 286/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0553 - val_loss: 0.0533\n",
      "Epoch 287/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0550 - val_loss: 0.0560\n",
      "Epoch 288/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0548 - val_loss: 0.0558\n",
      "Epoch 289/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0551 - val_loss: 0.0557\n",
      "Epoch 290/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0548 - val_loss: 0.0540\n",
      "Epoch 291/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0547 - val_loss: 0.0537\n",
      "Epoch 292/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0547 - val_loss: 0.0549\n",
      "Epoch 293/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0547 - val_loss: 0.0579\n",
      "Epoch 294/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0550 - val_loss: 0.0567\n",
      "Epoch 295/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0547 - val_loss: 0.0547\n",
      "Epoch 296/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0547 - val_loss: 0.0577\n",
      "Epoch 297/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0546 - val_loss: 0.0549\n",
      "Epoch 298/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0546 - val_loss: 0.0552\n",
      "Epoch 299/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0548 - val_loss: 0.0555\n",
      "Epoch 300/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0548 - val_loss: 0.0532\n",
      "Epoch 301/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0546 - val_loss: 0.0529\n",
      "Epoch 302/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0545 - val_loss: 0.0535\n",
      "Epoch 303/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0542 - val_loss: 0.0601\n",
      "Epoch 304/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0543 - val_loss: 0.0530\n",
      "Epoch 305/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0540 - val_loss: 0.0539\n",
      "Epoch 306/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0540 - val_loss: 0.0536\n",
      "Epoch 307/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0544 - val_loss: 0.0551\n",
      "Epoch 308/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0542 - val_loss: 0.0552\n",
      "Epoch 309/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0545 - val_loss: 0.0651\n",
      "Epoch 310/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0544 - val_loss: 0.0541\n",
      "Epoch 311/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0543 - val_loss: 0.0562\n",
      "Epoch 312/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0541 - val_loss: 0.0532\n",
      "Epoch 313/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0541 - val_loss: 0.0545\n",
      "Epoch 314/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0538 - val_loss: 0.0576\n",
      "Epoch 315/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0543 - val_loss: 0.0552\n",
      "Epoch 316/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0539 - val_loss: 0.0541\n",
      "Epoch 317/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0542 - val_loss: 0.0611\n",
      "Epoch 318/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0539 - val_loss: 0.0518\n",
      "Epoch 319/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0544 - val_loss: 0.0536\n",
      "Epoch 320/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0537 - val_loss: 0.0532\n",
      "Epoch 321/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0537 - val_loss: 0.0529\n",
      "Epoch 322/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0536 - val_loss: 0.0528\n",
      "Epoch 323/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0537 - val_loss: 0.0553\n",
      "Epoch 324/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0539 - val_loss: 0.0607\n",
      "Epoch 325/400\n",
      "1055/1055 [==============================] - 3s 2ms/step - loss: 0.0539 - val_loss: 0.0538\n",
      "Epoch 326/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0536 - val_loss: 0.0533\n",
      "Epoch 327/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0541 - val_loss: 0.0533\n",
      "Epoch 328/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0542 - val_loss: 0.0544\n",
      "Epoch 329/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0539 - val_loss: 0.0603\n",
      "Epoch 330/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0535 - val_loss: 0.0537\n",
      "Epoch 331/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0539 - val_loss: 0.0554\n",
      "Epoch 332/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0537 - val_loss: 0.0514\n",
      "Epoch 333/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0535 - val_loss: 0.0563\n",
      "Epoch 334/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0534 - val_loss: 0.0561\n",
      "Epoch 335/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0537 - val_loss: 0.0521\n",
      "Epoch 336/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0540 - val_loss: 0.0525\n",
      "Epoch 337/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0538 - val_loss: 0.0603\n",
      "Epoch 338/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0534 - val_loss: 0.0534\n",
      "Epoch 339/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0534 - val_loss: 0.0559\n",
      "Epoch 340/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0536 - val_loss: 0.0547\n",
      "Epoch 341/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0536 - val_loss: 0.0531\n",
      "Epoch 342/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0537 - val_loss: 0.0534\n",
      "Epoch 343/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0538 - val_loss: 0.0549\n",
      "Epoch 344/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0535 - val_loss: 0.0525\n",
      "Epoch 345/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0537 - val_loss: 0.0513\n",
      "Epoch 346/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0536 - val_loss: 0.0526\n",
      "Epoch 347/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0539 - val_loss: 0.0525\n",
      "Epoch 348/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0537 - val_loss: 0.0523\n",
      "Epoch 349/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0533 - val_loss: 0.0526\n",
      "Epoch 350/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0533 - val_loss: 0.0540\n",
      "Epoch 351/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0538 - val_loss: 0.0546\n",
      "Epoch 352/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0536 - val_loss: 0.0522\n",
      "Epoch 353/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0534 - val_loss: 0.0521\n",
      "Epoch 354/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0536 - val_loss: 0.0532\n",
      "Epoch 355/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0532 - val_loss: 0.0528\n",
      "Epoch 356/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0535 - val_loss: 0.0599\n",
      "Epoch 357/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0539 - val_loss: 0.0536\n",
      "Epoch 358/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0530 - val_loss: 0.0538\n",
      "Epoch 359/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0534 - val_loss: 0.0532\n",
      "Epoch 360/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0532 - val_loss: 0.0545\n",
      "Epoch 361/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0535 - val_loss: 0.0547\n",
      "Epoch 362/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0531 - val_loss: 0.0523\n",
      "Epoch 363/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0534 - val_loss: 0.0526\n",
      "Epoch 364/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0533 - val_loss: 0.0519\n",
      "Epoch 365/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0534 - val_loss: 0.0528\n",
      "Epoch 366/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0531 - val_loss: 0.0544\n",
      "Epoch 367/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0532 - val_loss: 0.0538\n",
      "Epoch 368/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0532 - val_loss: 0.0564\n",
      "Epoch 369/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0530 - val_loss: 0.0531\n",
      "Epoch 370/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0531 - val_loss: 0.0523\n",
      "Epoch 371/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0532 - val_loss: 0.0536\n",
      "Epoch 372/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0531 - val_loss: 0.0519\n",
      "Epoch 373/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0532 - val_loss: 0.0552\n",
      "Epoch 374/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0530 - val_loss: 0.0531\n",
      "Epoch 375/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0531 - val_loss: 0.0521\n",
      "Epoch 376/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0530 - val_loss: 0.0519\n",
      "Epoch 377/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0528 - val_loss: 0.0522\n",
      "Epoch 378/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0534 - val_loss: 0.0568\n",
      "Epoch 379/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0534 - val_loss: 0.0554\n",
      "Epoch 380/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0529 - val_loss: 0.0531\n",
      "Epoch 381/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0528 - val_loss: 0.0650\n",
      "Epoch 382/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0531 - val_loss: 0.0519\n",
      "Epoch 383/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0532 - val_loss: 0.0536\n",
      "Epoch 384/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0531 - val_loss: 0.0571\n",
      "Epoch 385/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0529 - val_loss: 0.0531\n",
      "Epoch 386/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0530 - val_loss: 0.0519\n",
      "Epoch 387/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0533 - val_loss: 0.0548\n",
      "Epoch 388/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0530 - val_loss: 0.0525\n",
      "Epoch 389/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0530 - val_loss: 0.0526\n",
      "Epoch 390/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0530 - val_loss: 0.0585\n",
      "Epoch 391/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0528 - val_loss: 0.0513\n",
      "Epoch 392/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0531 - val_loss: 0.0515\n",
      "Epoch 393/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0529 - val_loss: 0.0561\n",
      "Epoch 394/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0530 - val_loss: 0.0548\n",
      "Epoch 395/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0528 - val_loss: 0.0522\n",
      "Epoch 396/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0528 - val_loss: 0.0539\n",
      "Epoch 397/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0531 - val_loss: 0.0525\n",
      "Epoch 398/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0528 - val_loss: 0.0522\n",
      "Epoch 399/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0530 - val_loss: 0.0550\n",
      "Epoch 400/400\n",
      "1055/1055 [==============================] - 3s 3ms/step - loss: 0.0526 - val_loss: 0.0526\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 9)                 198       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 18)                180       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 18)                342       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 9)                 171       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 10        \n",
      "=================================================================\n",
      "Total params: 901\n",
      "Trainable params: 901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.fit(x=X_train,y=y_train,\n",
    "          validation_data=(X_test,y_test),\n",
    "          batch_size=128,epochs=400)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5e30a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.16792568439334454\n",
      "MSE: 0.05262438934781176\n",
      "RMSE: 0.22940006396645088\n",
      "VarScore: 0.77246495820706\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x293c9819370>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "from sklearn import metrics\n",
    "print('MAE:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('MSE:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('VarScore:',metrics.explained_variance_score(y_test,y_pred))\n",
    "# Visualizing Our predictions\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.scatter(y_test,y_pred)\n",
    "# Perfect predictions\n",
    "plt.plot(y_test,y_test,'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f102a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.560682</td>\n",
       "      <td>5.940048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.886104</td>\n",
       "      <td>5.851091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.700480</td>\n",
       "      <td>4.871927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.645447</td>\n",
       "      <td>5.389945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.356709</td>\n",
       "      <td>4.198393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.394449</td>\n",
       "      <td>4.383874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5.545177</td>\n",
       "      <td>5.486043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.919981</td>\n",
       "      <td>4.872437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.298317</td>\n",
       "      <td>5.311046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5.337538</td>\n",
       "      <td>5.470512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.208590</td>\n",
       "      <td>6.089840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5.198497</td>\n",
       "      <td>5.355030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.068904</td>\n",
       "      <td>5.171013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.595120</td>\n",
       "      <td>4.657779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.181784</td>\n",
       "      <td>5.127778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.308268</td>\n",
       "      <td>5.132421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.176150</td>\n",
       "      <td>5.066642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4.343805</td>\n",
       "      <td>4.208941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6.056784</td>\n",
       "      <td>6.061814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.793014</td>\n",
       "      <td>5.342872</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual  Predicted\n",
       "0   5.560682   5.940048\n",
       "1   5.886104   5.851091\n",
       "2   4.700480   4.871927\n",
       "3   5.645447   5.389945\n",
       "4   4.356709   4.198393\n",
       "5   4.394449   4.383874\n",
       "6   5.545177   5.486043\n",
       "7   4.919981   4.872437\n",
       "8   5.298317   5.311046\n",
       "9   5.337538   5.470512\n",
       "10  6.208590   6.089840\n",
       "11  5.198497   5.355030\n",
       "12  5.068904   5.171013\n",
       "13  4.595120   4.657779\n",
       "14  5.181784   5.127778\n",
       "15  5.308268   5.132421\n",
       "16  5.176150   5.066642\n",
       "17  4.343805   4.208941\n",
       "18  6.056784   6.061814\n",
       "19  5.793014   5.342872"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_2 = []\n",
    "for pred in y_pred:\n",
    "  y_pred_2.append(pred[0])\n",
    "\n",
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_2})\n",
    "df1 = df.head(20)\n",
    "df1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
